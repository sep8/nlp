{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from transformers import TFAlbertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/'\n",
    "local_zip = data_root + 'msra.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('../../data/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(data_dir=data_root + 'msra/', type='train'):\n",
    "    if type in ['train', 'val', 'test']:\n",
    "        sentences_path = os.path.join(data_dir, type, 'sentences.txt')\n",
    "        tags_path = os.path.join(data_dir, type, 'tags.txt')\n",
    "        return sentences_path, tags_path\n",
    "    else:\n",
    "        raise ValueError(\"data type not in ['train', 'val', 'test']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "batch_size = 32\n",
    "model_name = 'clue/albert_chinese_tiny'\n",
    "saved_model = 'ner_albert_tiny_text_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "class Sentence_Tokenizer(object):\n",
    "    def __init__(self, model_name, max_length=128, padded_token=True):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.padded_token = padded_token\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def bert_pack_inputs(self, sentences):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence, self.padded_token)\n",
    "            input_ids.append(tokens['input_ids'])\n",
    "            attention_masks.append(tokens['attention_mask'])\n",
    "            token_type_ids.append(tokens['token_type_ids'])\n",
    "        return [tf.constant(input_ids), tf.constant(token_type_ids), tf.constant(attention_masks)]\n",
    "    \n",
    "    def tokenize(self, sentence, padded_token=True):\n",
    "        padiding = 'max_length' if padded_token else True\n",
    "        tokens = self.tokenizer(text=sentence.strip(), max_length=self.max_length, truncation=True, padding=padiding, add_special_tokens=True)\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        words = self.tokenizer.decode(tokens)\n",
    "        return words\n",
    "\n",
    "tokenizer = Sentence_Tokenizer(model_name, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 13:37:46.560123: W tensorflow_text/core/kernels/fast_wordpiece_tokenizer_model_builder.cc:362] The empty string is found in the vocabulary, which takes place in the token id space but will never be used in the result. Consider cleaning it from the vocabulary.\n",
      "2022-06-21 13:37:46.560765: W tensorflow_text/core/kernels/fast_wordpiece_tokenizer_model_builder.cc:374] The empty suffix token is found in the vocabulary, which takes place in token id space but will (almost) never be used in the result. Consider cleaning it from the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self, vocab_path, seq_length):\n",
    "        super().__init__()\n",
    "        self.tokenize = tfm.nlp.layers.FastWordpieceBertTokenizer(\n",
    "            vocab_file=vocab_path,\n",
    "            lower_case=True)\n",
    "        self.bert_pack_inputs = tfm.nlp.layers.BertPackInputs(\n",
    "            seq_length=seq_length,\n",
    "            special_tokens_dict=self.tokenize.get_special_tokens_dict())\n",
    "\n",
    "vocab_path = './vocab.txt'\n",
    "preprocessor = Preprocessor(vocab_path, seq_length=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "encoder = TFAlbertModel.from_pretrained(model_name, from_pt=True, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n",
      "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 34 65 38 24 29 16 32 50  5 48 27 13  4  7 37  3 17  2 19 37 12 16 16\n",
      " 38 28 25 24 28 31  6 24 49 46  3 10 47 37 12 40 39 13 23 12 29 44 47 36\n",
      " 23 20 48 16 24  5 21 36 16 48  6 24 46  7 10 36 14 30 11  5 41 27  1  0\n",
      " 18 47 23 48 37  4 10 19 45 34 54 31 34 55  9 32 36 42  5 38 14 38  1 49\n",
      " 36 26 37  3 13  2 46 30 23 11 31 25  0  7 22 46  4 42 25 49 35 47 29  6\n",
      " 36 40  7 45 23 24 29 37 29  2 23 30 49 34 23 42 58 13 25 12  2 24 32 16\n",
      " 53 47 46 24 65  2 28 49 12 16 12 18  5 38 38  6 32 37 12  8 35  9 25 43\n",
      " 40 14 27 12 11 16  9 12 65 18 42 16 15 43 49 37  5 21 11  4 22 15 24 35\n",
      " 32 26 30 39 25 36 40 38 35  5 25  2 32 40 19 13 38 11 24 45 12 40  5 12\n",
      " 34 13 23 37  3 39  6 17 25 26 43  2  6 14 23  5 27 18 19  2 37  4 28 24\n",
      " 47  2 32 37 36 49  7 44 38 25 24  1 27 25 44 37 46 19 22 19 36 28 24 22\n",
      " 25 23 44 48 16  2 26 17  6  4  1  1 41 39 17  1 22 11 30  6 32 24 16 16\n",
      " 44 23  4 47 43 23 24 44 34 22 35 23 16 28 39  2 14  3 32 31  2 41 32 24]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_inputs = tf.constant([text_input])\n",
    "tokenize = hub.KerasLayer(preprocessor.tokenize, trainable=True)\n",
    "bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, trainable=True)\n",
    "encoder_inputs = bert_pack_inputs([tokenize(text_inputs)])\n",
    "\n",
    "# print(encoder_inputs['input_word_ids'])\n",
    "\n",
    "outputs = encoder(\n",
    "    input_ids=encoder_inputs['input_word_ids'], token_type_ids=encoder_inputs['input_type_ids'], attention_mask=encoder_inputs['input_mask']\n",
    ")\n",
    "outputs = np.argmax(outputs[0], 1)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 34 68 38  2 35 16 34 43  5 23 27 13  4  7 37  3 30 20 14 37 12 16 16\n",
      " 38 28 35 34 28 51  6 24 49 19  7 10 52 37 12 40 39 13 23 12 29 44 47 36\n",
      " 23 46 20 20 24  5 21 36 16 48 34 24 46  7 10 34 14 25 19  5 41 26  1  0\n",
      " 18 47 23 48 38  4 23 19 45 34 53 31 34 48  9 29 36 42 28 38  9  4  1 49\n",
      " 34 26 37  3 13  2 46 30 23 11 31 25  0  7 12 35  4 42 46 49 10 47 43 19\n",
      " 36 40 19 45 23 24 18 37 29  2 23 30 49 11 23 42 58 10 37 12 23 39  7 16\n",
      " 11 47 46 23 64  2 28 49 20 16 12 18  5 38 38  6 32 37 12  8 35  9 35 22\n",
      " 40 14 39 12 11 24 39 16 64  8 42 16 35 43 49 37  5 21 22  4 29 15 24 35\n",
      " 47 26 30 24 23 46 40 38 25  5 15  8 33  5 19 13 38 11 25 45 12 40  5 12\n",
      " 34 13 23 37  3 39  6 35 37 26 43  2  6 18 23  5 27 11 19  2 37  4 28 24\n",
      " 47  2 32 37  5 49  7 44 38 25 24  1 27 17 44 37 46 24 22 19 36 28 24 44\n",
      " 25 23 44 48 16  2 26 17  6  4 36  1 41 27 30 37 34 11 30  6 32 24 16 16\n",
      " 44 23 17 45 43  7 24 44 34  7 35 29 16 28 39  2 19  3 39 31 35 41 32 24]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = tokenizer.bert_pack_inputs([text_input])\n",
    "# print(encoder_inputs[0])\n",
    "\n",
    "outputs = encoder(\n",
    "    input_ids=encoder_inputs[0], token_type_ids=encoder_inputs[1], attention_mask=encoder_inputs[2]\n",
    ")\n",
    "outputs = np.argmax(outputs[0], 1)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('c3-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022b350d9d731ea6beeec17b0760db9de374177ccae69f8fd03b1c527dbc4257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
