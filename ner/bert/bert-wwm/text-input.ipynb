{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from transformers import TFAlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/'\n",
    "local_zip = data_root + 'msra.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('../../data/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(data_dir=data_root + 'msra/', type='train'):\n",
    "    if type in ['train', 'val', 'test']:\n",
    "        sentences_path = os.path.join(data_dir, type, 'sentences.txt')\n",
    "        tags_path = os.path.join(data_dir, type, 'tags.txt')\n",
    "        return sentences_path, tags_path\n",
    "    else:\n",
    "        raise ValueError(\"data type not in ['train', 'val', 'test']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "model_name = 'voidful/albert_chinese_base'\n",
    "saved_model = 'tr_ner_albert_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label_Tokenizer(object):\n",
    "    def __init__(self, labels, max_length):\n",
    "        super().__init__()\n",
    "        self.size = len(labels)\n",
    "        labels_to_ids = {k: v for v, k in enumerate(labels)}\n",
    "        ids_to_labels = {v: k for v, k in enumerate(labels)}\n",
    "        self.labels_to_ids = labels_to_ids\n",
    "        self.ids_to_labels = ids_to_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, labels):\n",
    "        tokens = [self._tokenize(label) for label in labels]\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize(self, label):\n",
    "        label = label.decode('utf-8') if hasattr(label, 'decode') else label\n",
    "        labels = [le for le in label.strip().split(' ')]\n",
    "        special_token = self.encode(['O'])[0]\n",
    "\n",
    "        tokens = self.encode(labels)\n",
    "        tokens = tokens[:self.max_length - 2]\n",
    "        tokens = [special_token] + tokens + [special_token]\n",
    "        # Add padded TAG tokens\n",
    "        padding_len = self.max_length - len(tokens)\n",
    "        tokens = tokens + ([special_token] * padding_len)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, labels):\n",
    "        return [self.labels_to_ids[label] for label in labels]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.ids_to_labels[id] for id in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'B-ORG', 'I-PER', 'B-PER', 'I-LOC', 'I-ORG', 'B-LOC']\n",
    "label_tokenizer = Label_Tokenizer(labels, max_length=max_len)\n",
    "labels_num = label_tokenizer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 21:59:57.475481: W tensorflow_text/core/kernels/fast_wordpiece_tokenizer_model_builder.cc:362] The empty string is found in the vocabulary, which takes place in the token id space but will never be used in the result. Consider cleaning it from the vocabulary.\n",
      "2022-06-20 21:59:57.475953: W tensorflow_text/core/kernels/fast_wordpiece_tokenizer_model_builder.cc:374] The empty suffix token is found in the vocabulary, which takes place in token id space but will (almost) never be used in the result. Consider cleaning it from the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self, vocab_path, seq_length):\n",
    "        super().__init__()\n",
    "        self.tokenize = tfm.nlp.layers.FastWordpieceBertTokenizer(\n",
    "            vocab_file=vocab_path,\n",
    "            lower_case=True)\n",
    "        self.bert_pack_inputs = tfm.nlp.layers.BertPackInputs(\n",
    "            seq_length=seq_length,\n",
    "            special_tokens_dict=self.tokenize.get_special_tokens_dict())\n",
    "\n",
    "vocab_path = './albert_base_vocab.txt'\n",
    "preprocessor = Preprocessor(vocab_path, seq_length=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_data(type, max_lines=None):\n",
    "  sentences_file, labels_file = get_data_path(type=type)\n",
    "  sentences = tf.data.TextLineDataset(sentences_file).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  if(max_lines != None):\n",
    "    sentences = sentences.take(max_lines)\n",
    "  x = [sentence for sentence in sentences.as_numpy_iterator()]\n",
    "  x = tf.constant(x)\n",
    "\n",
    "  labels = tf.data.TextLineDataset(labels_file).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  if(max_lines != None):\n",
    "    labels = labels.take(max_lines)\n",
    "  labels = [label.decode('utf-8') for label in labels.as_numpy_iterator()]\n",
    "  y = label_tokenizer.tokenize(labels)\n",
    "  y = tf.constant(y)\n",
    "  print(len(y))\n",
    "  return x, y\n",
    "\n",
    "def get_dataset(max_lines=None):\n",
    "  val_max_lines = None\n",
    "  if(max_lines != None):\n",
    "    val_max_lines = int(max_lines / 10)\n",
    "  train = get_type_data('train', max_lines)\n",
    "  val = get_type_data('val', val_max_lines)\n",
    "  test = get_type_data('test', val_max_lines)\n",
    "  return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n",
      "3000\n",
      "3442\n"
     ]
    }
   ],
   "source": [
    "train, val, test = get_dataset()\n",
    "x_train, y_train = train\n",
    "x_val, y_val = val\n",
    "x_test, y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(labels_num):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "    tokenize = hub.KerasLayer(preprocessor.tokenize, trainable=False)\n",
    "    bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, trainable=False)\n",
    "    encoder_inputs = bert_pack_inputs([tokenize(text_input)])\n",
    "\n",
    "    encoder = TFAlbertModel.from_pretrained(model_name, from_pt=True)\n",
    "    outputs = encoder(\n",
    "        input_ids=encoder_inputs['input_word_ids'], token_type_ids=encoder_inputs['input_type_ids'], attention_mask=encoder_inputs['input_mask']\n",
    "    )\n",
    "\n",
    "    embedding = outputs[0]\n",
    "    embedding = keras.layers.Dropout(0.3)(embedding)\n",
    "    logits = keras.layers.Dense(\n",
    "        labels_num + 1, activation='softmax', name='NER')(embedding)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[text_input],\n",
    "        outputs=[logits],\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_2 (KerasLayer)     (None, None, None)   0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " keras_layer_3 (KerasLayer)     {'input_word_ids':   0           ['keras_layer_2[0][0]']          \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " tf_albert_model_1 (TFAlbertMod  TFBaseModelOutputWi  10547968   ['keras_layer_3[0][2]',          \n",
      " el)                            thPooling(last_hidd               'keras_layer_3[0][0]',          \n",
      "                                en_state=(None, 128               'keras_layer_3[0][1]']          \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 128, 768)     0           ['tf_albert_model_1[0][0]']      \n",
      "                                                                                                  \n",
      " NER (Dense)                    (None, 128, 8)       6152        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,554,120\n",
      "Trainable params: 10,554,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(labels_num)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ming8525/miniforge3/envs/c3-nlp/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model_1/albert/pooler/kernel:0', 'tf_albert_model_1/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model_1/albert/pooler/kernel:0', 'tf_albert_model_1/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model_1/albert/pooler/kernel:0', 'tf_albert_model_1/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model_1/albert/pooler/kernel:0', 'tf_albert_model_1/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 183/1313 [===>..........................] - ETA: 2:17:13 - loss: 0.1847 - accuracy: 0.9475"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "No debugger available, can not send 'disconnect'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=4,\n",
    "    verbose=1,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text, model):\n",
    "    x_test = tf.constant([text])\n",
    "    pred_test = model.predict(x_test) if hasattr(model, 'predict') else model(x_test)\n",
    "    # ignore predictions of padding tokens\n",
    "    pred_tags = np.argmax(pred_test, 2)[0]\n",
    "\n",
    "    tags = label_tokenizer.decode(pred_tags)[:len(text) + 2]\n",
    "    res = []\n",
    "    words = {\n",
    "        'word': '',\n",
    "        'tag': None\n",
    "    }\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if(tag != 'O'):\n",
    "            pre, suf = tag.split('-')\n",
    "            words['tag'] = suf\n",
    "            word = text[idx-1]\n",
    "            # word = tokenizer.decode(token)\n",
    "            # word = preprocessor.decode(word)\n",
    "            words['word'] = words['word'] + word if words['word'] else word\n",
    "        else:\n",
    "            if(words['tag']):\n",
    "                res.append(words)\n",
    "            words = {\n",
    "                'word': '',\n",
    "                'tag': None\n",
    "            }\n",
    "    return pd.DataFrame(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_text(test_inputs, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saved_model, include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_model = tf.saved_model.load(saved_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_text(test_inputs, reload_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '无症状感染者28：现住朝阳区慧苑商住公寓，曾到访天堂超市酒吧(工体西路6号)，6月13日诊断为无症状感染者'\n",
    "\n",
    "print(predict_from_text(text, reload_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ad98a190b4b5f35ccc183fea6461b9c6eca3817fd9b9f811f8bcf53cbfc1ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
