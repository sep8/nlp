{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import BertTokenizer, TFAlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/'\n",
    "local_zip = data_root + 'msra.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('../../data/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(data_dir=data_root + 'msra/', type='train'):\n",
    "    if type in ['train', 'val', 'test']:\n",
    "        sentences_path = os.path.join(data_dir, type, 'sentences.txt')\n",
    "        tags_path = os.path.join(data_dir, type, 'tags.txt')\n",
    "        return sentences_path, tags_path\n",
    "    else:\n",
    "        raise ValueError(\"data type not in ['train', 'val', 'test']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "model_name = 'clue/albert_chinese_tiny'\n",
    "saved_model = 'tr_ner_albert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label_Tokenizer(object):\n",
    "    def __init__(self, labels, max_length):\n",
    "        super().__init__()\n",
    "        self.size = len(labels)\n",
    "        labels_to_ids = {k: v for v, k in enumerate(labels)}\n",
    "        ids_to_labels = {v: k for v, k in enumerate(labels)}\n",
    "        self.labels_to_ids = labels_to_ids\n",
    "        self.ids_to_labels = ids_to_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, labels):\n",
    "        tokens = [self._tokenize(label) for label in labels]\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize(self, label):\n",
    "        label = label.decode('utf-8') if hasattr(label, 'decode') else label\n",
    "        labels = [le for le in label.strip().split(' ')]\n",
    "        pad_token = self.encode(['[PAD]'])[0]\n",
    "        special_token = self.encode(['O'])[0]\n",
    "\n",
    "        tokens = self.encode(labels)\n",
    "        tokens = tokens[:self.max_length - 2]\n",
    "        tokens = [special_token] + tokens + [special_token]\n",
    "        # Add padded TAG tokens\n",
    "        padding_len = self.max_length - len(tokens)\n",
    "        tokens = tokens + ([pad_token] * padding_len)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, labels):\n",
    "        return [self.labels_to_ids[label] for label in labels]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.ids_to_labels[id] for id in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'B-ORG', 'I-PER', 'B-PER', 'I-LOC', 'I-ORG', 'B-LOC', '[PAD]']\n",
    "label_tokenizer = Label_Tokenizer(labels, max_length=max_len)\n",
    "labels_num = label_tokenizer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Tokenizer(object):\n",
    "    def __init__(self, model_name, max_length=128, padded_token=True):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.padded_token = padded_token\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def bert_pack_inputs(self, sentences):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence, self.padded_token)\n",
    "            input_ids.append(tokens['input_ids'])\n",
    "            attention_masks.append(tokens['attention_mask'])\n",
    "            token_type_ids.append(tokens['token_type_ids'])\n",
    "        return [tf.constant(input_ids), tf.constant(token_type_ids), tf.constant(attention_masks)]\n",
    "    \n",
    "    def tokenize(self, sentence, padded_token=True):\n",
    "        padiding = 'max_length' if padded_token else True\n",
    "        tokens = self.tokenizer(text=sentence.strip(), max_length=self.max_length, truncation=True, padding=padiding, add_special_tokens=True)\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        words = self.tokenizer.decode(tokens)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Sentence_Tokenizer(model_name, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file, tags_file = get_data_path()\n",
    "sentences = tf.data.TextLineDataset(sentences_file).take(1600)\n",
    "sentences = [sentence.decode('utf-8') for sentence in sentences.as_numpy_iterator()]\n",
    "x_train = tokenizer.bert_pack_inputs(sentences)\n",
    "\n",
    "labels = tf.data.TextLineDataset(tags_file).take(1600)\n",
    "labels = [label.decode('utf-8') for label in labels.as_numpy_iterator()]\n",
    "y_train = label_tokenizer.tokenize(labels)\n",
    "y_train = tf.constant(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file, tags_file = get_data_path(type='val')\n",
    "sentences = tf.data.TextLineDataset(sentences_file).take(160)\n",
    "sentences = [sentence.decode('utf-8') for sentence in sentences.as_numpy_iterator()]\n",
    "x_val = tokenizer.bert_pack_inputs(sentences)\n",
    "\n",
    "labels = tf.data.TextLineDataset(tags_file).take(160)\n",
    "labels = [label.decode('utf-8') for label in labels.as_numpy_iterator()]\n",
    "y_val = label_tokenizer.tokenize(labels)\n",
    "y_val = tf.constant(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "pad_token = label_tokenizer.encode(['[PAD]'])[0]\n",
    "\n",
    "# Each batch of data will consist of variable sized sentence tokens with\n",
    "# appropriate padding in both input and target.\n",
    "# During loss calculation, we ignore the loss corresponding padding tokens\n",
    "# in the target.\n",
    "\n",
    "\n",
    "def masked_ce_loss(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # tf.math.equal([pad], pad_token) => True\n",
    "    # logical_not(True)-> False, cast(False) -> 0\n",
    "    # loss_ *= mask, ignore the loss corresponding padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, pad_token))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(labels_num, max_lenth):\n",
    "    input_ids = keras.layers.Input(shape=(max_lenth,), dtype=tf.int32)\n",
    "    token_type_ids = keras.layers.Input(shape=(max_lenth,), dtype=tf.int32)\n",
    "    attention_mask = keras.layers.Input(shape=(max_lenth,), dtype=tf.int32)\n",
    "\n",
    "    encoder = TFAlbertModel.from_pretrained(model_name, from_pt=True)\n",
    "    outputs = encoder(\n",
    "        input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    embedding = outputs[0]\n",
    "    embedding = keras.layers.Dropout(0.3)(embedding)\n",
    "    tag_logits = keras.layers.Dense(\n",
    "        labels_num, activation='softmax', name='NER')(embedding)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[tag_logits],\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(optimizer=optimizer, loss=masked_ce_loss,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_albert_model_1 (TFAlbertMod  TFBaseModelOutputWi  4080520    ['input_13[0][0]',               \n",
      " el)                            thPooling(last_hidd               'input_15[0][0]',               \n",
      "                                en_state=(None, 128               'input_14[0][0]']               \n",
      "                                , 312),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 312),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128, 312)     0           ['tf_albert_model_1[0][0]']      \n",
      "                                                                                                  \n",
      " NER (Dense)                    (None, 128, 8)       2504        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,083,024\n",
      "Trainable params: 4,083,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(labels_num, max_lenth=max_len)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 51s 1s/step - loss: 0.1451 - accuracy: 0.3503 - val_loss: 0.0874 - val_accuracy: 0.3659\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 56s 1s/step - loss: 0.0822 - accuracy: 0.3600 - val_loss: 0.0603 - val_accuracy: 0.3746\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 58s 1s/step - loss: 0.0605 - accuracy: 0.3670 - val_loss: 0.0493 - val_accuracy: 0.3793\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=3,\n",
    "    verbose=1,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text, model):\n",
    "    n_tokens = len(tokenizer.tokenize(text, padded_token=False)['input_ids'])\n",
    "    tokens = tokenizer.bert_pack_inputs([text])\n",
    "    x_test = tokens\n",
    "    pred_test = model.predict(x_test) if hasattr(model, 'predict') else model(x_test)\n",
    "    # ignore predictions of padding tokens\n",
    "    pred_tags = np.argmax(pred_test, 2)[0][:n_tokens]\n",
    "\n",
    "    tags = label_tokenizer.decode(pred_tags)\n",
    "    res = []\n",
    "    words = {\n",
    "        'word': '',\n",
    "        'tag': None\n",
    "    }\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if(tag != 'O' and tag != '[PAD]'):\n",
    "            pre, suf = tag.split('-')\n",
    "            words['tag'] = suf\n",
    "            token = x_test[0][0][idx]\n",
    "            token = token.numpy()\n",
    "            word = tokenizer.decode(token)\n",
    "            # word = preprocessor.decode(word)\n",
    "            words['word'] = words['word'] + word if words['word'] else word\n",
    "        else:\n",
    "            if(words['tag']):\n",
    "                res.append(words)\n",
    "            words = {\n",
    "                'word': '',\n",
    "                'tag': None\n",
    "            }\n",
    "    return pd.DataFrame(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "             word  tag\n",
      "0              李华  PER\n",
      "1  朝阳区香河园街道西坝河北里社  LOC\n",
      "2           天安门广场  ORG\n",
      "3          太阳宫凯德茂  PER\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_text(test_inputs, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tr_ner_albert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tr_ner_albert/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(saved_model, include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_model = tf.saved_model.load(saved_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  tag\n",
      "0              李华  PER\n",
      "1  朝阳区香河园街道西坝河北里社  LOC\n",
      "2           天安门广场  ORG\n",
      "3          太阳宫凯德茂  PER\n"
     ]
    }
   ],
   "source": [
    "print(predict_from_text(test_inputs, reload_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('c3-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ad98a190b4b5f35ccc183fea6461b9c6eca3817fd9b9f811f8bcf53cbfc1ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
