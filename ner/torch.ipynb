{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from transformers import AlbertModel, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.backends.mps.is_available()\n",
    "device = torch.device('mps' if use_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/'\n",
    "local_zip = data_root + 'ner_datasets.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(data_root)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "batch_size = 32\n",
    "model_name = 'clue/albert_chinese_tiny'\n",
    "saved_model = './models/ner_albert_chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(dataset='mara', type='train'):\n",
    "    data_dir = data_root + 'ner_datasets/' + dataset\n",
    "    if type in ['train', 'val', 'test'] and dataset in ['msra', 'daily', 'weibo']:\n",
    "        sentences = os.path.join(data_dir, type, 'sentences.txt')\n",
    "        labels = os.path.join(data_dir, type, 'labels.txt')\n",
    "        return sentences, labels\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"data type not in ['train', 'val', 'test'] or dataset name not in ['msra', 'daily']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines(filename):\n",
    "    with open(filename) as file:\n",
    "        lines = [line.rstrip() for line in file]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label_Tokenizer(object):\n",
    "    def __init__(self, labels, max_length):\n",
    "        super().__init__()\n",
    "        self.size = len(labels)\n",
    "        labels_to_ids = {k: v for v, k in enumerate(labels)}\n",
    "        ids_to_labels = {v: k for v, k in enumerate(labels)}\n",
    "        self.labels_to_ids = labels_to_ids\n",
    "        self.ids_to_labels = ids_to_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, labels):\n",
    "        tokens = [self._tokenize(label) for label in labels]\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize(self, label):\n",
    "        label = label.decode('utf-8') if hasattr(label, 'decode') else label\n",
    "        labels = [le for le in label.split(' ')]\n",
    "        special_token = self.encode(['O'])[0]\n",
    "\n",
    "        tokens = self.encode(labels)\n",
    "        tokens = tokens[:self.max_length - 2]\n",
    "        tokens = [special_token] + tokens + [special_token]\n",
    "        # Add padded TAG tokens\n",
    "        padding_len = self.max_length - len(tokens)\n",
    "        tokens = tokens + ([special_token] * padding_len)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, labels):\n",
    "        return [self.labels_to_ids[label] for label in labels]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.ids_to_labels[id] for id in ids]\n",
    "\n",
    "\n",
    "labels = ['O', 'B-ORG', 'I-PER', 'B-PER', 'I-LOC', 'I-ORG', 'B-LOC']\n",
    "label_tokenizer = Label_Tokenizer(labels, max_length=max_len)\n",
    "num_labels = label_tokenizer.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "class Sentence_Tokenizer(object):\n",
    "    def __init__(self, model_name, max_length=128, padded_token=True):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.padded_token = padded_token\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "    def bert_pack_inputs(self, sentences):\n",
    "        outputs = [self.tokenize(sentence, self.padded_token) for sentence in sentences]\n",
    "        return outputs\n",
    "\n",
    "    def tokenize(self, sentence, padded_token=True):\n",
    "        padiding = 'max_length' if padded_token else True\n",
    "        tokens = self.tokenizer(text=sentence, max_length=self.max_length, truncation=True, padding=padiding, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        words = self.tokenizer.decode(tokens)\n",
    "        return words\n",
    "\n",
    "\n",
    "tokenizer = Sentence_Tokenizer(model_name, max_length=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "  def __init__(self, type = 'train', datasets = ['msra', 'daily'], max_line = None):\n",
    "    x_data = None\n",
    "    y_data = None\n",
    "    for dataset in datasets:\n",
    "        sen_file, labels_file = get_data_path(dataset, type)\n",
    "        sentences = get_file_lines(sen_file)\n",
    "        labels = get_file_lines(labels_file)\n",
    "        if(max_line is not None):\n",
    "          sentences = sentences[:max_line]\n",
    "          labels = labels[:max_line]\n",
    "        x_data = sentences if x_data is None else [*x_data, *sentences]\n",
    "        y_data = labels if y_data is None else [*y_data, *labels]\n",
    "\n",
    "    self.x_data = tokenizer.bert_pack_inputs(x_data)\n",
    "    self.y_data = torch.tensor(label_tokenizer.tokenize(y_data)).long()\n",
    "    self.len = len(y_data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.x_data[index], self.y_data[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset('train')\n",
    "test_dataset = NERDataset('val')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at clue/albert_chinese_tiny were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "         super(Net, self).__init__()\n",
    "         self.num_labels = num_labels\n",
    "         self.encoder = AlbertModel.from_pretrained(model_name)\n",
    "         self.bert_drop = torch.nn.Dropout(0.1)\n",
    "         self.liner1 = torch.nn.Linear(312, 128)\n",
    "         self.liner2 = torch.nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "         outputs = self.encoder(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "         logits = outputs[0] #(32,150, 312)\n",
    "         logits = self.bert_drop(logits)\n",
    "         logits = self.liner1(logits)\n",
    "         logits = self.liner2(logits)\n",
    "         return logits\n",
    "\n",
    "model = Net(num_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "def loss_fn(outputs, labels, num_labels):\n",
    "    return criterion(outputs.view(-1, num_labels), labels.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    inputs, target = data\n",
    "    input_ids = inputs['input_ids'].squeeze(1).to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].squeeze(1).to(device)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "    target = target.long().to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(input_ids,token_type_ids, attention_mask)\n",
    "    loss = loss_fn(logits, target, model.num_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if i % 300 == 299:\n",
    "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 300))\n",
    "      running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(logits, labels): #(32, 150, 7) (32,150)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    batch_size = logits.size(0)\n",
    "  \n",
    "    for i in range(batch_size):\n",
    "        predicted = logits[i].argmax(dim=1)[labels[i] != 0]\n",
    "\n",
    "        label = labels[i][labels[i] != 0]\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    return correct / total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  total_acc = 0\n",
    "  with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "      inputs, labels = data\n",
    "      input_ids = inputs['input_ids'].squeeze(1).to(device)\n",
    "      token_type_ids = inputs['token_type_ids'].squeeze(1).to(device)\n",
    "      attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "      labels = labels.long().to(device)\n",
    "      \n",
    "      logits = model(input_ids,token_type_ids, attention_mask)\n",
    "      \n",
    "      acc = accuracy_fn(logits, labels)\n",
    "      total_acc += acc\n",
    "\n",
    "  print('Accuracy of the network on the test inputs: %d %%' % (100 * total_acc / len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.433\n",
      "[1,   600] loss: 0.158\n",
      "[1,   900] loss: 0.144\n",
      "[1,  1200] loss: 0.134\n",
      "[1,  1500] loss: 0.125\n",
      "[1,  1800] loss: 0.122\n",
      "Accuracy of the network on the test inputs: 17 %\n",
      "[2,   300] loss: 0.115\n",
      "[2,   600] loss: 0.107\n",
      "[2,   900] loss: 0.106\n",
      "[2,  1200] loss: 0.103\n",
      "[2,  1500] loss: 0.101\n",
      "[2,  1800] loss: 0.097\n",
      "Accuracy of the network on the test inputs: 16 %\n",
      "[3,   300] loss: 0.090\n",
      "[3,   600] loss: 0.090\n",
      "[3,   900] loss: 0.089\n",
      "[3,  1200] loss: 0.088\n",
      "[3,  1500] loss: 0.084\n",
      "[3,  1800] loss: 0.084\n",
      "Accuracy of the network on the test inputs: 16 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(sentence):\n",
    "  \n",
    "    tokenized_inputs = tokenizer.tokenize(sentence)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(0)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(0)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "    text = tokenizer.tokenize(sentence)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_id = text['input_ids'].to(device)\n",
    "        type_id = text['token_type_ids'].to(device)\n",
    "        mask = text['attention_mask'].to(device)\n",
    "\n",
    "        label_ids = torch.Tensor(align_word_ids(sentence)).to(device)\n",
    "\n",
    "        logits = model(input_id, type_id, mask)\n",
    "        predictions = logits[0].argmax(dim=1)\n",
    "        predictions = predictions[label_ids != 0]\n",
    "\n",
    "        input_ids = input_id[0][label_ids != 0].tolist()\n",
    "        prediction_label = label_tokenizer.decode(predictions.tolist())\n",
    "    \n",
    "        res = []\n",
    "        words = {\n",
    "            'word': '',\n",
    "            'tag': None\n",
    "        }\n",
    "        for idx, tag in enumerate(prediction_label):\n",
    "            if(tag != 'O'):\n",
    "                _, suf = tag.split('-')\n",
    "                words['tag'] = suf\n",
    "                token = input_ids[idx]\n",
    "                word = tokenizer.decode(token)\n",
    "                words['word'] = words['word'] + word if words['word'] else word\n",
    "            else:\n",
    "                if(words['tag']):\n",
    "                    res.append(words)\n",
    "                words = {\n",
    "                    'word': '',\n",
    "                    'tag': None\n",
    "                }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '李华住在朝阳区香河园街道西坝河北里社区，在5', 'tag': 'ORG'},\n",
       " {'word': '4号去', 'tag': 'ORG'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input= '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'\n",
    "\n",
    "evaluate_one_text(model, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f525c695730c841e052b8d34cf6e8bcfdb8d8f78b4a6432d240c7bfc8c210784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
