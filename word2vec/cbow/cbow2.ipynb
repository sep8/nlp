{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import pad_sequences\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # numbers\n",
    "    \"5 2 4 8 6 2 3 6 4\",\n",
    "    \"4 8 5 6 9 5 5 6\",\n",
    "    \"1 1 5 2 3 3 8\",\n",
    "    \"3 6 9 6 8 7 4 6 3\",\n",
    "    \"8 9 9 6 1 4 3 4\",\n",
    "    \"1 0 2 0 2 1 3 3 3 3 3\",\n",
    "    \"9 3 3 0 1 4 7 8\",\n",
    "    \"9 9 8 5 6 7 1 2 3 0 1 0\",\n",
    "\n",
    "    # alphabets, expecting that 9 is close to letters\n",
    "    \"a t g q e h 9 u f\",\n",
    "    \"e q y u o i p s\",\n",
    "    \"q o 9 p l k j o k k o p\",\n",
    "    \"h g y i u t t a e q\",\n",
    "    \"i k d q r e 9 e a d\",\n",
    "    \"o p d g 9 s a f g a\",\n",
    "    \"i u y g h k l a s w\",\n",
    "    \"o l u y a o g f s\",\n",
    "    \"o p i u y g d a s j d l\",\n",
    "    \"u k i l o 9 l j s\",\n",
    "    \"y g i s h k j l f r f\",\n",
    "    \"i o h n 9 9 d 9 f a 9\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line.split(' ') for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in sentence] for sentence in corpus]\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "window_size = 2\n",
    "context_length = window_size*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 31\n",
      "Vocabulary Sample: [('9', 1), ('3', 2), ('o', 3), ('6', 4), ('a', 5), ('1', 6), ('g', 7), ('i', 8), ('4', 9), ('8', 10)]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size):\n",
    "    pairs = []\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = context_words\n",
    "            y = label_word\n",
    "            pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_context_word_pairs(wids, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 9]] [15]\n",
      "[[15, 9, 10]] [16]\n",
      "[[15, 16, 10, 4]] [9]\n",
      "[[16, 9, 4, 16]] [10]\n",
      "[[9, 10, 16, 2]] [4]\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs[:5]:\n",
    "  print(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 16  9]] tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]], shape=(1, 31), dtype=float32)\n",
      "[[ 0 15  9 10]] tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]], shape=(1, 31), dtype=float32)\n",
      "[[15 16 10  4]] tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]], shape=(1, 31), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 18:20:25.105081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for pair in pairs:\n",
    "  xs.append(pad_sequences(pair[0], maxlen=context_length))\n",
    "  ys.append(tf.one_hot(pair[1], vocab_size))\n",
    "\n",
    "for idx, x in enumerate(xs[:3]):\n",
    "  print(x, ys[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(1, 4), dtype=tf.int32, name=None), TensorSpec(shape=(1, 31), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = 1000\n",
    "dataset = tf.data.Dataset.from_tensor_slices((xs, ys))\n",
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.embedding = keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=window_size*2,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.mean = keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1), output_shape=(embedding_dim,))\n",
    "    self.dense = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "  def call(self, context):\n",
    "    word_emb = self.embedding(context)\n",
    "    word_emb = self.mean(word_emb)\n",
    "    pred = self.dense(word_emb)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vocab_size, embed_size)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ming8525/miniforge3/envs/c3-nlp/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 1s 922us/step - loss: 3.4169 - accuracy: 0.0725\n",
      "Epoch 2/100\n",
      "193/193 [==============================] - 0s 779us/step - loss: 3.2554 - accuracy: 0.1917\n",
      "Epoch 3/100\n",
      "193/193 [==============================] - 0s 933us/step - loss: 3.0242 - accuracy: 0.2383\n",
      "Epoch 4/100\n",
      "193/193 [==============================] - 0s 774us/step - loss: 2.7792 - accuracy: 0.2694\n",
      "Epoch 5/100\n",
      "193/193 [==============================] - 0s 690us/step - loss: 2.5922 - accuracy: 0.2850\n",
      "Epoch 6/100\n",
      "193/193 [==============================] - 0s 816us/step - loss: 2.4474 - accuracy: 0.3057\n",
      "Epoch 7/100\n",
      "193/193 [==============================] - 0s 661us/step - loss: 2.3253 - accuracy: 0.2902\n",
      "Epoch 8/100\n",
      "193/193 [==============================] - 0s 891us/step - loss: 2.2193 - accuracy: 0.3057\n",
      "Epoch 9/100\n",
      "193/193 [==============================] - 0s 710us/step - loss: 2.1261 - accuracy: 0.3161\n",
      "Epoch 10/100\n",
      "193/193 [==============================] - 0s 1ms/step - loss: 2.0436 - accuracy: 0.3420\n",
      "Epoch 11/100\n",
      "193/193 [==============================] - 0s 799us/step - loss: 1.9697 - accuracy: 0.3627\n",
      "Epoch 12/100\n",
      "193/193 [==============================] - 0s 679us/step - loss: 1.9032 - accuracy: 0.3782\n",
      "Epoch 13/100\n",
      "193/193 [==============================] - 0s 743us/step - loss: 1.8428 - accuracy: 0.3938\n",
      "Epoch 14/100\n",
      "193/193 [==============================] - 0s 731us/step - loss: 1.7876 - accuracy: 0.4041\n",
      "Epoch 15/100\n",
      "193/193 [==============================] - 0s 769us/step - loss: 1.7369 - accuracy: 0.4249\n",
      "Epoch 16/100\n",
      "193/193 [==============================] - 0s 799us/step - loss: 1.6901 - accuracy: 0.4456\n",
      "Epoch 17/100\n",
      "193/193 [==============================] - 0s 742us/step - loss: 1.6468 - accuracy: 0.4611\n",
      "Epoch 18/100\n",
      "193/193 [==============================] - 0s 755us/step - loss: 1.6065 - accuracy: 0.4663\n",
      "Epoch 19/100\n",
      "193/193 [==============================] - 0s 729us/step - loss: 1.5688 - accuracy: 0.4870\n",
      "Epoch 20/100\n",
      "193/193 [==============================] - 0s 735us/step - loss: 1.5336 - accuracy: 0.4922\n",
      "Epoch 21/100\n",
      "193/193 [==============================] - 0s 755us/step - loss: 1.5006 - accuracy: 0.5130\n",
      "Epoch 22/100\n",
      "193/193 [==============================] - 0s 722us/step - loss: 1.4695 - accuracy: 0.5337\n",
      "Epoch 23/100\n",
      "193/193 [==============================] - 0s 724us/step - loss: 1.4401 - accuracy: 0.5389\n",
      "Epoch 24/100\n",
      "193/193 [==============================] - 0s 743us/step - loss: 1.4123 - accuracy: 0.5440\n",
      "Epoch 25/100\n",
      "193/193 [==============================] - 0s 768us/step - loss: 1.3860 - accuracy: 0.5648\n",
      "Epoch 26/100\n",
      "193/193 [==============================] - 0s 817us/step - loss: 1.3610 - accuracy: 0.5803\n",
      "Epoch 27/100\n",
      "193/193 [==============================] - 0s 811us/step - loss: 1.3373 - accuracy: 0.5855\n",
      "Epoch 28/100\n",
      "193/193 [==============================] - 0s 737us/step - loss: 1.3147 - accuracy: 0.5803\n",
      "Epoch 29/100\n",
      "193/193 [==============================] - 0s 724us/step - loss: 1.2932 - accuracy: 0.5855\n",
      "Epoch 30/100\n",
      "193/193 [==============================] - 0s 726us/step - loss: 1.2727 - accuracy: 0.5855\n",
      "Epoch 31/100\n",
      "193/193 [==============================] - 0s 687us/step - loss: 1.2531 - accuracy: 0.5855\n",
      "Epoch 32/100\n",
      "193/193 [==============================] - 0s 705us/step - loss: 1.2343 - accuracy: 0.5907\n",
      "Epoch 33/100\n",
      "193/193 [==============================] - 0s 729us/step - loss: 1.2164 - accuracy: 0.5907\n",
      "Epoch 34/100\n",
      "193/193 [==============================] - 0s 739us/step - loss: 1.1992 - accuracy: 0.6062\n",
      "Epoch 35/100\n",
      "193/193 [==============================] - 0s 713us/step - loss: 1.1828 - accuracy: 0.6114\n",
      "Epoch 36/100\n",
      "193/193 [==============================] - 0s 760us/step - loss: 1.1670 - accuracy: 0.6166\n",
      "Epoch 37/100\n",
      "193/193 [==============================] - 0s 726us/step - loss: 1.1519 - accuracy: 0.6373\n",
      "Epoch 38/100\n",
      "193/193 [==============================] - 0s 911us/step - loss: 1.1373 - accuracy: 0.6477\n",
      "Epoch 39/100\n",
      "193/193 [==============================] - 0s 726us/step - loss: 1.1233 - accuracy: 0.6425\n",
      "Epoch 40/100\n",
      "193/193 [==============================] - 0s 776us/step - loss: 1.1099 - accuracy: 0.6528\n",
      "Epoch 41/100\n",
      "193/193 [==============================] - 0s 774us/step - loss: 1.0970 - accuracy: 0.6580\n",
      "Epoch 42/100\n",
      "193/193 [==============================] - 0s 777us/step - loss: 1.0845 - accuracy: 0.6580\n",
      "Epoch 43/100\n",
      "193/193 [==============================] - 0s 737us/step - loss: 1.0726 - accuracy: 0.6632\n",
      "Epoch 44/100\n",
      "193/193 [==============================] - 0s 791us/step - loss: 1.0610 - accuracy: 0.6684\n",
      "Epoch 45/100\n",
      "193/193 [==============================] - 0s 771us/step - loss: 1.0499 - accuracy: 0.6684\n",
      "Epoch 46/100\n",
      "193/193 [==============================] - 0s 690us/step - loss: 1.0391 - accuracy: 0.6684\n",
      "Epoch 47/100\n",
      "193/193 [==============================] - 0s 795us/step - loss: 1.0288 - accuracy: 0.6684\n",
      "Epoch 48/100\n",
      "193/193 [==============================] - 0s 685us/step - loss: 1.0188 - accuracy: 0.6736\n",
      "Epoch 49/100\n",
      "193/193 [==============================] - 0s 717us/step - loss: 1.0091 - accuracy: 0.6736\n",
      "Epoch 50/100\n",
      "193/193 [==============================] - 0s 752us/step - loss: 0.9997 - accuracy: 0.6736\n",
      "Epoch 51/100\n",
      "193/193 [==============================] - 0s 778us/step - loss: 0.9907 - accuracy: 0.6736\n",
      "Epoch 52/100\n",
      "193/193 [==============================] - 0s 672us/step - loss: 0.9819 - accuracy: 0.6736\n",
      "Epoch 53/100\n",
      "193/193 [==============================] - 0s 793us/step - loss: 0.9735 - accuracy: 0.6736\n",
      "Epoch 54/100\n",
      "193/193 [==============================] - 0s 783us/step - loss: 0.9653 - accuracy: 0.6788\n",
      "Epoch 55/100\n",
      "193/193 [==============================] - 0s 815us/step - loss: 0.9573 - accuracy: 0.6839\n",
      "Epoch 56/100\n",
      "193/193 [==============================] - 0s 662us/step - loss: 0.9496 - accuracy: 0.6839\n",
      "Epoch 57/100\n",
      "193/193 [==============================] - 0s 815us/step - loss: 0.9421 - accuracy: 0.6839\n",
      "Epoch 58/100\n",
      "193/193 [==============================] - 0s 658us/step - loss: 0.9348 - accuracy: 0.6839\n",
      "Epoch 59/100\n",
      "193/193 [==============================] - 0s 783us/step - loss: 0.9278 - accuracy: 0.6995\n",
      "Epoch 60/100\n",
      "193/193 [==============================] - 0s 694us/step - loss: 0.9209 - accuracy: 0.6995\n",
      "Epoch 61/100\n",
      "193/193 [==============================] - 0s 950us/step - loss: 0.9143 - accuracy: 0.6995\n",
      "Epoch 62/100\n",
      "193/193 [==============================] - 0s 665us/step - loss: 0.9078 - accuracy: 0.6995\n",
      "Epoch 63/100\n",
      "193/193 [==============================] - 0s 816us/step - loss: 0.9015 - accuracy: 0.6995\n",
      "Epoch 64/100\n",
      "193/193 [==============================] - 0s 684us/step - loss: 0.8954 - accuracy: 0.7098\n",
      "Epoch 65/100\n",
      "193/193 [==============================] - 0s 774us/step - loss: 0.8894 - accuracy: 0.7098\n",
      "Epoch 66/100\n",
      "193/193 [==============================] - 0s 664us/step - loss: 0.8836 - accuracy: 0.7098\n",
      "Epoch 67/100\n",
      "193/193 [==============================] - 0s 819us/step - loss: 0.8780 - accuracy: 0.7098\n",
      "Epoch 68/100\n",
      "193/193 [==============================] - 0s 727us/step - loss: 0.8724 - accuracy: 0.7098\n",
      "Epoch 69/100\n",
      "193/193 [==============================] - 0s 696us/step - loss: 0.8670 - accuracy: 0.7098\n",
      "Epoch 70/100\n",
      "193/193 [==============================] - 0s 725us/step - loss: 0.8618 - accuracy: 0.7098\n",
      "Epoch 71/100\n",
      "193/193 [==============================] - 0s 747us/step - loss: 0.8567 - accuracy: 0.7098\n",
      "Epoch 72/100\n",
      "193/193 [==============================] - 0s 740us/step - loss: 0.8516 - accuracy: 0.7150\n",
      "Epoch 73/100\n",
      "193/193 [==============================] - 0s 792us/step - loss: 0.8467 - accuracy: 0.7202\n",
      "Epoch 74/100\n",
      "193/193 [==============================] - 0s 761us/step - loss: 0.8420 - accuracy: 0.7306\n",
      "Epoch 75/100\n",
      "193/193 [==============================] - 0s 796us/step - loss: 0.8373 - accuracy: 0.7358\n",
      "Epoch 76/100\n",
      "193/193 [==============================] - 0s 732us/step - loss: 0.8327 - accuracy: 0.7358\n",
      "Epoch 77/100\n",
      "193/193 [==============================] - 0s 831us/step - loss: 0.8282 - accuracy: 0.7358\n",
      "Epoch 78/100\n",
      "193/193 [==============================] - 0s 682us/step - loss: 0.8238 - accuracy: 0.7358\n",
      "Epoch 79/100\n",
      "193/193 [==============================] - 0s 854us/step - loss: 0.8195 - accuracy: 0.7358\n",
      "Epoch 80/100\n",
      "193/193 [==============================] - 0s 762us/step - loss: 0.8153 - accuracy: 0.7358\n",
      "Epoch 81/100\n",
      "193/193 [==============================] - 0s 787us/step - loss: 0.8112 - accuracy: 0.7358\n",
      "Epoch 82/100\n",
      "193/193 [==============================] - 0s 732us/step - loss: 0.8072 - accuracy: 0.7409\n",
      "Epoch 83/100\n",
      "193/193 [==============================] - 0s 726us/step - loss: 0.8032 - accuracy: 0.7409\n",
      "Epoch 84/100\n",
      "193/193 [==============================] - 0s 846us/step - loss: 0.7993 - accuracy: 0.7409\n",
      "Epoch 85/100\n",
      "193/193 [==============================] - 0s 696us/step - loss: 0.7955 - accuracy: 0.7409\n",
      "Epoch 86/100\n",
      "193/193 [==============================] - 0s 757us/step - loss: 0.7917 - accuracy: 0.7409\n",
      "Epoch 87/100\n",
      "193/193 [==============================] - 0s 680us/step - loss: 0.7881 - accuracy: 0.7409\n",
      "Epoch 88/100\n",
      "193/193 [==============================] - 0s 730us/step - loss: 0.7844 - accuracy: 0.7461\n",
      "Epoch 89/100\n",
      "193/193 [==============================] - 0s 668us/step - loss: 0.7809 - accuracy: 0.7461\n",
      "Epoch 90/100\n",
      "193/193 [==============================] - 0s 781us/step - loss: 0.7774 - accuracy: 0.7461\n",
      "Epoch 91/100\n",
      "193/193 [==============================] - 0s 728us/step - loss: 0.7740 - accuracy: 0.7461\n",
      "Epoch 92/100\n",
      "193/193 [==============================] - 0s 742us/step - loss: 0.7706 - accuracy: 0.7461\n",
      "Epoch 93/100\n",
      "193/193 [==============================] - 0s 679us/step - loss: 0.7673 - accuracy: 0.7461\n",
      "Epoch 94/100\n",
      "193/193 [==============================] - 0s 679us/step - loss: 0.7640 - accuracy: 0.7513\n",
      "Epoch 95/100\n",
      "193/193 [==============================] - 0s 851us/step - loss: 0.7608 - accuracy: 0.7513\n",
      "Epoch 96/100\n",
      "193/193 [==============================] - 0s 715us/step - loss: 0.7577 - accuracy: 0.7513\n",
      "Epoch 97/100\n",
      "193/193 [==============================] - 0s 819us/step - loss: 0.7546 - accuracy: 0.7513\n",
      "Epoch 98/100\n",
      "193/193 [==============================] - 0s 755us/step - loss: 0.7515 - accuracy: 0.7565\n",
      "Epoch 99/100\n",
      "193/193 [==============================] - 0s 761us/step - loss: 0.7485 - accuracy: 0.7617\n",
      "Epoch 100/100\n",
      "193/193 [==============================] - 0s 729us/step - loss: 0.7455 - accuracy: 0.7617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e701b8eb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_w2v_word_embedding(model, path):\n",
    "    word_emb = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "    # word_emb = model.get_weights()[0]\n",
    "    word_emb = word_emb[1:]\n",
    "    for i in range(vocab_size -1):\n",
    "        c = \"blue\"\n",
    "        try:\n",
    "            int(id2word[i])\n",
    "        except ValueError:\n",
    "            c = \"red\"\n",
    "        plt.text(word_emb[i, 0], word_emb[i, 1], s=id2word[i], color=c, weight=\"bold\")\n",
    "    plt.xlim(word_emb[:, 0].min() - .5, word_emb[:, 0].max() + .5)\n",
    "    plt.ylim(word_emb[:, 1].min() - .5, word_emb[:, 1].max() + .5)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlabel(\"embedding dim1\")\n",
    "    plt.ylabel(\"embedding dim2\")\n",
    "    plt.savefig(path, dpi=300, format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "show_w2v_word_embedding(word2vec, './cbow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "# out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "# out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# for index in tokenizer.index_word:\n",
    "#   if index == 0:\n",
    "#     continue  # skip 0, it's padding.\n",
    "#   word = tokenizer.index_word[index]\n",
    "#   vec = weights[index]\n",
    "#   out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "#   out_m.write(word + \"\\n\")\n",
    "# out_v.close()\n",
    "# out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('c3-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ad98a190b4b5f35ccc183fea6461b9c6eca3817fd9b9f811f8bcf53cbfc1ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
