{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tenor implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 2, 0, 2],\n",
    "    [1, 1, 1, 1]\n",
    "]).float()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "w_key = torch.tensor([[0, 0, 1],\n",
    "                   [1, 1, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [1, 1, 0]]).float()\n",
    "\n",
    "w_query = torch.tensor([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]]).float()\n",
    "w_value = torch.tensor([[0, 2, 0],\n",
    "                   [0, 3, 0],\n",
    "                   [1, 0, 3],\n",
    "                   [1, 1, 0]]).float()\n",
    "\n",
    "print(w_key.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]])\n",
      "querys tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]])\n",
      "values tensor([[1., 2., 3.],\n",
      "        [2., 8., 0.],\n",
      "        [2., 6., 3.]])\n",
      "shape torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "keys = x @ w_key\n",
    "print('keys', keys)\n",
    "\n",
    "querys = x @ w_query\n",
    "print('querys', keys)\n",
    "\n",
    "values = x @ w_value\n",
    "print('values', values)\n",
    "\n",
    "print('shape', keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  4.],\n",
      "        [ 4., 16., 12.],\n",
      "        [ 4., 12., 10.]])\n",
      "tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
      "        [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
      "        [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = querys @ keys.t()\n",
    "print(attn_scores)\n",
    "attn_scores_softmax = F.softmax(attn_scores, dim=1)\n",
    "print(attn_scores_softmax)\n",
    "print(attn_scores_softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9366, 6.6831, 1.5951],\n",
      "        [2.0000, 7.9640, 0.0540],\n",
      "        [1.9997, 7.7599, 0.3584]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "weighted_values = attn_scores_softmax @ values\n",
    "print(weighted_values)\n",
    "print(weighted_values.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            @ w_query(4,3) -> Q(3,3)\n",
    "                                     -> softmat(Q@K.t()) -> attn_scores(3,3)\n",
    "input(3,4)  @ w_key(4,3)   -> K(3,3)\n",
    "                                                                        -> attn_scores @ V -> weighted_values(3,3)\n",
    "            @ w_value(4,3) -> V(3,3) ----------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch implementtation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    # input : (batch_size,(seq_len,input_dim)\n",
    "    # q : (batch_size,input_dim,dim_k)\n",
    "    # k : (batch_size,input_dim,dim_k)\n",
    "    # v : (batch_size,input_dim,dim_v)\n",
    "    def __init__(self,input_dim, dim_k,dim_v):\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x) # Q: (batch_size,seq_len,dim_k)\n",
    "        K = self.k(x) # K: (batch_size,seq_len,dim_k)\n",
    "        V = self.v(x) # V: (batch_size,seq_len,dim_v)\n",
    "         \n",
    "        score = torch.bmm(Q,K.permute(0,2,1)) * self._norm_fact #(batch_size, seq_len, seq_len)\n",
    "        score = F.softmax(score, dim=-1) # (batch_size, seq_len, seq_len)\n",
    "        atten = torch.bmm(score,V) # (batch_size, seq_len, dim_v)\n",
    "\n",
    "        return atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-size torch.Size([4, 3, 2])\n",
      "output-size torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_len = 3\n",
    "input_dim = 2\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "print('input-size', x.size())\n",
    "\n",
    "dim_k = 4\n",
    "dim_v = 5\n",
    "self_attn = Self_Attention(input_dim, dim_k, dim_v)\n",
    "output = self_attn(x)\n",
    "print('output-size', output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muti-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "\t# calculate attention using function we will define next\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "            \n",
    "        output = torch.matmul(scores, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "heads = 2\n",
    "d_models = 4\n",
    "\n",
    "x = torch.randn(3, 4, 4)\n",
    "multihead_attn = MultiHeadAttention(heads, d_models)\n",
    "output = multihead_attn(x, x, x)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f525c695730c841e052b8d34cf6e8bcfdb8d8f78b4a6432d240c7bfc8c210784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
