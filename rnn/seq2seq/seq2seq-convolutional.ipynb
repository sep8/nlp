{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def get_translation_texts(input_file='data/deu.txt', output_file='data/en-de.txt'):\n",
    "#   regex = r\"\\tCC-BY 2.0.+$\"\n",
    "#   lines = open(input_file, encoding='utf-8').read().strip().split('\\n')\n",
    "#   lines = [re.sub(regex, '', line) for line in lines]\n",
    "#   file = open(output_file,'w')\n",
    "#   for line in lines:\n",
    "#     file.write(line+\"\\n\")\n",
    "#   file.close()\n",
    "\n",
    "# get_translation_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111bbbdf0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "max_lines = 10000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang1_vocab_vocab_size 12030\n",
      "lang2_vocab_size 23470\n"
     ]
    }
   ],
   "source": [
    "lang1 = 'en'\n",
    "lang2 = 'de'\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "def lang1_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [token.lower() for token in en_tokenizer(sentence)]\n",
    "\n",
    "def lang2_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [token for token in de_tokenizer(sentence)]\n",
    "\n",
    "def build_vocab(lang1='en', lang2='de'):\n",
    "    counter1 = Counter()\n",
    "    counter2 = Counter()\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    for l in lines:\n",
    "        l1, l2 = l.split('\\t')\n",
    "        counter1.update(lang1_tokenizer(l1))\n",
    "        counter2.update(lang2_tokenizer(l2))\n",
    "\n",
    "    vocab1 = vocab(counter1, min_freq=2, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "    vocab2 = vocab(counter2, min_freq=2, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "    vocab1.set_default_index(vocab1[\"<unk>\"])\n",
    "    vocab2.set_default_index(vocab2[\"<unk>\"])\n",
    "    return [vocab1, vocab2]\n",
    "\n",
    "lang1_vocab, lang2_vocab = build_vocab(lang1, lang2)\n",
    "lang1_vocab_size = len(lang1_vocab)\n",
    "lang2_vocab_size = len(lang2_vocab)\n",
    "print('lang1_vocab_vocab_size', lang1_vocab_size)\n",
    "print('lang2_vocab_size', lang2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, lang1, lang2, max_lines, device=torch.device(\"cpu\")):\n",
    "        self.lang1 = lang1\n",
    "        self.lang2 = lang2\n",
    "        self.device = device\n",
    "        self.data = []\n",
    "        self.untokenized_data = []\n",
    "        lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "        lines.reverse() #use longer data\n",
    "        num_lines = 0\n",
    "        for l in lines:\n",
    "            l1, l2 = l.split('\\t')\n",
    "            if (max_lines is None or max_lines > num_lines):\n",
    "                l1_tokens = self.tokenize_sentence(l1, True)\n",
    "                l2_tokens = self.tokenize_sentence(l2, False)\n",
    "                self.data.append((l1_tokens, l2_tokens))\n",
    "                self.untokenized_data.append((l1, l2))\n",
    "                num_lines += 1\n",
    "\n",
    "        self.len = len(self.data)\n",
    "\n",
    "    def tokenize_sentence(self, sentence, is_lang1):\n",
    "        vocab = lang1_vocab if is_lang1 else lang2_vocab\n",
    "        tokenizer = lang1_tokenizer if is_lang1 else lang2_tokenizer\n",
    "        indexes = [vocab[token] for token in tokenizer(sentence)]\n",
    "        indexes = [vocab['<sos>']] + indexes + [vocab['<eos>']]\n",
    "        return torch.tensor(indexes, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 10000\n"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset(lang1, lang2, max_lines, device)\n",
    "print('length of dataset', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_loader 500\n",
      "length of val_loader 63\n",
      "length of test_loader 63\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    source = [item[0] for item in batch] \n",
    "    #pad them using pad_sequence method from pytorch. \n",
    "    source = pad_sequence(source, batch_first=True, padding_value=lang1_vocab['<pad>']) \n",
    "    \n",
    "    #get all target indexed sentences of the batch\n",
    "    target = [item[1] for item in batch] \n",
    "    #pad them using pad_sequence method from pytorch. \n",
    "    target = pad_sequence(target, batch_first=True, padding_value=lang2_vocab['<pad>'])\n",
    "    return source, target\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('length of train_loader', len(train_loader))\n",
    "print('length of val_loader', len(val_loader))\n",
    "print('length of test_loader', len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 24]) torch.Size([16, 24])\n",
      "[tensor([[    2,   834,   546,   181,  1333,    41,  1376,  1338,   882,   979,\n",
      "          1775,   237,   820,   237,  1842,   148, 11994,     5,     3,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  1213,  3211,   834,  1828,    93,    18,   115,   588,   287,\n",
      "            69,   289,   159,  2033,  2599,   148,   207,   422,    47,   228,\n",
      "           111,   484,     5,     3],\n",
      "        [    2,   834,  1843,  8029,  7055,   244,  1313,  2781,  6596,   834,\n",
      "           371,   601,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   258,   422,  1353,  1005,  1730,   128,   316,    36,   284,\n",
      "            80,   271,    34,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   148,   738,   402,  6557,   454,  9934,  5760,    47,  5208,\n",
      "           402,   834,  4693,     5,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  8369,   979,  4254,   235,   683,   593,  5805,    93,  4165,\n",
      "          4565,   334,   607,   420,    36,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    22,   336,   316,   284,   406,   834,  5099,   690,   272,\n",
      "           690,   128,   701,   700,    18,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   834,   990,    64,   284,    12,    92,   289,   284,  7886,\n",
      "            47,   989,  1495,  7273,  5201,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    67,  1730,   159,   169,   736,    69,   284,    80,  2368,\n",
      "            67,   179,   159,   332,   158,   208,     5,     3,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    69,   333,  7611,   284,  7233,  1326,    47,   815,   259,\n",
      "          2027,   521, 11501,   284,  4239,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    22,   172,   284,   311,   518,   253,   882,    93,  4165,\n",
      "           237,  1730,   159,   133,  1338,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   284,   834,   990,   402,   593,  4484,    93,   163, 10624,\n",
      "           334,  4707,  7162,  2434, 11525,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    69,  1842,   148,  1119,   657,  5460,   228,  1765,   179,\n",
      "           159,   332,   158,   284,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  5880,   402,   834,   106,  3726,    93,   834,  1503,   333,\n",
      "          5067,  1287,  3831,     5,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  5698,    47,   163,  3404,   402,   834,  2330,   289,  7696,\n",
      "             0,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  1214,   690,   290,   690,   237,  1013,    93,   237,   153,\n",
      "           440,   284,   155,   834,   621,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1]])]\n",
      "[tensor([[    2,   114,  1006,   626,  2035,  2503,  1579,  5783,    45,   736,\n",
      "           181,  2258, 18422,   226,   238,  1052,     0,     5,     3,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  5124,  1821,  9534,    78,  3286,  2525,    45,   525,   349,\n",
      "            45,  1411,  4191,  5182,   718,   133,   681,   116,   600,  1342,\n",
      "           328,  1569,     5,     3],\n",
      "        [    2,  1762,  7123, 19522,   226,   349,    45,  1411,   238,   249,\n",
      "            63,   867,  2782,    77,   158,  2922,  1078, 12364,     5,     3,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  4077,   835,  1342,   697,  6029,   528,    45,  1411,   132,\n",
      "          2455,   541,  3558,   348,    51,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  5672,   867,  3417,   479,  2565, 11610,     0, 14876,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  7710,  5158,   736,  8550,  8576,    58,     0,    45,   119,\n",
      "          7765,   481,  1401,    34,     0,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  5124,   528,   158,  7325,   113,  2919,  2677,   287,    45,\n",
      "          1231,  1897,   132,   238,  1381,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   184,   158,  9485,  7348,   736,   121,  8082,   528,   195,\n",
      "          1232,  4989,    77,  3997, 15811,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   128,  4006,   133,   116,  3331,   113,   630,    45,  4121,\n",
      "           429,   237,   116,   394,   130,  5989,     5,     3,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   133,   226,  3403, 22732,  2554, 13913,   113,  4074,  9853,\n",
      "         15068, 13687,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    32,  2704,   779,  1579,  2375,    45,   209,   238,   622,\n",
      "          5282,    23,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2, 13636,   132,    91,    45,   525,  1957, 20226,  2737,    45,\n",
      "          1411,   249, 12817, 17216,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   133,   226,  2016,    45, 19750,  4191,  3143,  1405,   818,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2, 14283,   867,  4279,  4539,   355,   279,  2684,  2292,  7455,\n",
      "          4590,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   120,  4177,  6581,  5029, 12123,  2979,  5860,  1278,  9915,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2, 16856,   238,   600,   544,    37,    45,   835,   238,  2587,\n",
      "            45,  5597,   238,    78,  1121,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1]])]\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_loader):\n",
    "    print(item[0].shape, item[1].shape)\n",
    "    print([item[0]])\n",
    "    print([item[1]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        #input = (batch_size, seq_len)\n",
    "        batch_size = input.size(0)\n",
    "        seq_len = input.size(1)\n",
    "        \n",
    "        #create position tensor, pos = [0, 1, 2, 3, ..., seq_len - 1], pos = (batch_size, seq_len)\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        #embed tokens and positions, tok_embedded = pos_embedded = [batch_size, seq_len, emb_dim]\n",
    "        tok_embedded = self.tok_embedding(input)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #combine embeddings by elementwise summing, [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb_dim to hid_dim\n",
    "        conv_input = self.emb2hid(embedded) # [batch_size, seq_len, emb_dim]\n",
    "                \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) # [batch_size, hid_dim, seq_len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input)) # [batch_size, 2 * hid_dim, seq_len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1) # [batch_size, hid_dim, seq_len]\n",
    "\n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale #[batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb_dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1)) #[batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention # [seq_len, batch_size, emb_dim]\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch_size, seq_len, emb_dim]\n",
    "        #conved = [batch_size, hid_dim, seq_len]\n",
    "        #encoder_conved = encoder_combined = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        #permute and convert back to emb_dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1)) #[batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale #[batch_size, seq_len, emb_dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1)) # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2) # [batch_size, seq_len, seq_len]\n",
    "\n",
    "        attended_encoding = torch.matmul(attention, encoder_combined) # [batch_size, seq_len, emd_dim]\n",
    "        \n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding) # [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale # [batch_size, hid_dim, seq_len]\n",
    " \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, target, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch_size, seq_len]\n",
    "        #encoder_conved = encoder_combined = [batch_size, seq_len, emb_dim]\n",
    "                \n",
    "        batch_size = target.size(0)\n",
    "        seq_len = target.size(1)\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) #[batch_size, seq_len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(target) # [batch_size, seq_len, emb_dim]\n",
    "        pos_embedded = self.pos_embedding(pos) # [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded) # [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb_dim -> hid_dim\n",
    "        conv_input = self.emb2hid(embedded) # [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) # [batch_size, hid_dim, seq_len]\n",
    "        \n",
    "        batch_size = target.size(0)\n",
    "        hid_dim = conv_input.size(1)\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2) # [batch_size, hid_dim, seq_len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input) # [batch_size, 2 * hid_dim, seq_len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1) # [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            #calculate attention, [batch_size, seq_len, seq_len]\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale # [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1)) # [batch_size, seq_len, emb_dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved)) # [batch_size, seq_len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        #input = [batch_size, seq_len]\n",
    "        #target = [batch_size, trg_len-1] (<eos> token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) input embedding plus positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(input) # both are [batch_size, seq_len, emb dim]\n",
    "            \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the target sentence\n",
    "        #attention a batch of attention scores across the input sentence for each word in the target sentence\n",
    "        output, attention = self.decoder(target, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch_size, trg_len-1, output_dim], attention = [batch_size, trg_len-1, seq_len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = lang1_vocab_size\n",
    "OUTPUT_DIM = lang2_vocab_size\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = lang2_vocab['<pad>']\n",
    "max_length = 100\n",
    "model_path = 'models/seq2seq_cnn.pt'\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device, 100)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device, 100)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 47,437,486 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    for i, (input, target) in enumerate(iterator):\n",
    "        #input = [batch_size, seq_len]\n",
    "        #target = [batch_size, trg_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(input, target[:,:-1]) # [batch_size, trg_len-1, output_dim]\n",
    "        \n",
    "        output_dim = output.size(-1)\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim) # [batch_size * (trg_len-1), output_dim]\n",
    "        target = target[:,1:].contiguous().view(-1) # [batch_size * (trg_len - 1)]\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print_loss_total += loss.item()\n",
    "        if(i+1 % print_every == 0):\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (i, i / len(iterator) * 100, print_loss_avg))\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (input, target) in enumerate(iterator):\n",
    "            #input = [batch_size, seq_len]\n",
    "            #target = [batch_size, trg_len]\n",
    "\n",
    "            output, _ = model(input, target[:,:-1]) # [batch_size, trg_len-1, output_dim]\n",
    "        \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim) # [batch_size * (trg_len-1), output_dim]\n",
    "            target = target[:,1:].contiguous().view(-1) # [batch_size * (trg_len - 1)]\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 17s\n",
      "\tTrain Loss: 6.116\n",
      "\t Val. Loss: 5.581\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, max_length=max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = dataset.tokenize_sentence(sentence, True)\n",
    "        inputs = torch.LongTensor(inputs).unsqueeze(0).to(device)\n",
    "\n",
    "        encoder_conved, encoder_combined = model.encoder(inputs)\n",
    "\n",
    "        target = [lang2_vocab['<sos>']]\n",
    "\n",
    "        for i in range(max_length):\n",
    "\n",
    "            target_tensor = torch.LongTensor(target).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, attention = model.decoder(target_tensor, encoder_conved, encoder_combined)\n",
    "            \n",
    "            pred_token = output.argmax(2)[:,-1].item()\n",
    "            \n",
    "            target.append(pred_token)\n",
    "\n",
    "            if pred_token == lang2_vocab['<eos>']:\n",
    "                break\n",
    "        \n",
    "        trg_tokens = [lang2_vocab.get_itos()[i] for i in target]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> A new team was formed in order to take part in the boat race.\n",
      "= Ein neues Team wurde gegründet, um an dem Bootrennen teilzunehmen.\n",
      "< Ein du , dass Tom nicht , dass Tom nicht , dass die nicht , dass die nicht , um die nicht .\n",
      "\n",
      "> One thing I've always wanted to do is learn to fly an airplane.\n",
      "= Eine Sache, die ich immer tun wollte, ist, ein Flugzeug fliegen zu lernen.\n",
      "< Sie du , dass Tom nicht , dass Tom das , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass er das , um das , dass Tom das , dass Tom nicht , dass Tom das , dass Tom nicht , dass Tom nicht , dass Tom das , dass das nicht , dass Tom das , dass Tom nicht , dass das das , dass das nicht , dass Tom das , dass Tom nicht , dass Tom das , dass Tom nicht , dass Tom das , dass Tom nicht , dass\n",
      "\n",
      "> Excuse me, but would you please tell me the way to the post office?\n",
      "= Entschuldigen Sie bitte, könnten Sie mir den Weg zur Post zeigen?\n",
      "< Sie ist , dass es nicht , dass das das , dass das nicht , dass er das , um das .\n",
      "\n",
      "> One day this caterpillar will turn into a beautiful butterfly.\n",
      "= Aus dieser Raupe wird einmal ein schöner Schmetterling.\n",
      "< Sie ist , dass Tom nicht , dass Tom das , dass das nicht , dass das nicht , um das nicht .\n",
      "\n",
      "> An opportunity like this only comes along once in a lifetime.\n",
      "= So eine Chance gibt es nur einmal im Leben.\n",
      "< Sie ist , dass es nicht , dass es das , um das nicht , um es nicht .\n",
      "\n",
      "> Tom asked for Mary's address and wrote it down on a piece of scrap paper.\n",
      "= Tom fragte nach Marys Adresse und schrieb sie auf einen Zettel.\n",
      "< Tom hat nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass er nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht\n",
      "\n",
      "> I was able to convince Tom, but I wasn't able to convince Mary.\n",
      "= Tom konnte ich überzeugen, nicht aber Maria.\n",
      "< Ich weiß nicht , dass Tom nicht , dass ich das , , ich das , dass ich nicht , dass ich nicht , dass ich nicht , dass ich das , dass ich nicht , dass ich nicht , dass ich das , dass ich nicht , dass ich das , dass ich nicht , dass ich das , dass ich nicht , dass ich das , dass ich nicht , dass ich das , dass ich das , dass ich nicht , dass ich nicht , dass ich das , dass ich nicht , dass ich das\n",
      "\n",
      "> Dream as if you'll live forever. Live as if you'll die today.\n",
      "= Träume, als wenn das Leben ewig währte; lebe, als wenn es heute endete!\n",
      "< Tom du nicht , dass Tom nicht , dass er das , um , das das .\n",
      "\n",
      "> We had to get something to eat so we stopped at a restaurant near the exit.\n",
      "= Wir mussten etwas essen; deswegen hielten wir an einem Restaurant unweit der Autobahnausfahrt.\n",
      "< Wir du , dass Tom nicht , dass die das , um die nicht , um die nicht .\n",
      "\n",
      "> Tom made it very clear why we had to have the job finished by 2:30.\n",
      "= Tom gab uns sehr klar zu verstehen, warum wir bis 14.30 Uhr fertig werden mussten.\n",
      "< Tom hat nicht , dass Tom nicht , dass Tom nicht , dass er nicht , dass er nicht , dass Tom nicht , dass er nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht , dass Tom nicht\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluateRandomly(n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(dataset.untokenized_data)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        translation, _ = translate_sentence(pair[0])\n",
    "        translation = ' '.join(translation[:-1])\n",
    "        print('<', translation)\n",
    "        print('')\n",
    "evaluateRandomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> She took me under her wing and taught me everything she knows.\n",
      "< Sie ist , das Sie nicht , um Sie nicht , um sie nicht .\n"
     ]
    }
   ],
   "source": [
    "input = dataset.untokenized_data[7000][0]\n",
    "print('>', input)\n",
    "translation, attention = translate_sentence(input)\n",
    "translation_sentence = ' '.join(translation[:-1])\n",
    "print('<', translation_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/54/mjyp15vd0xx14_pf4h693504mkk7tr/T/ipykernel_14228/3761034016.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],\n",
      "/var/folders/54/mjyp15vd0xx14_pf4h693504mkk7tr/T/ipykernel_14228/3761034016.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+translation)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(input, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, max_length = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for (input, target) in data:\n",
    "        pred_trget, _ = translate_sentence(input, max_length=max_length)\n",
    "        target = lang2_tokenizer(target)\n",
    "        #cut off <eos> token\n",
    "        pred_trget = pred_trget[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trget)\n",
    "        trgs.append(target)\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 0.00\n"
     ]
    }
   ],
   "source": [
    "def calculate_data_bleu(n_num=100):\n",
    "    data = [random.choice(dataset.untokenized_data) for i in range(n_num)]\n",
    "    bleu_score = calculate_bleu(data)\n",
    "    return bleu_score\n",
    "\n",
    "score = calculate_data_bleu()\n",
    "print(f'BLEU score = {score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4670fb8ce984b94d35f99e9f6e3660635f3fa9b149816d0027b93d0ab56905c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
