{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "from utils import timeSince\n",
    "\n",
    "from cn_cn_data import get_cn_en_dataloader, to_sentence, cn_vocab, en_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_loader 2207\n"
     ]
    }
   ],
   "source": [
    "seq_len = 15\n",
    "max_length = seq_len + 2\n",
    "batch_size = 1\n",
    "\n",
    "cn_vocab_size = len(cn_vocab)\n",
    "en_vocab_size = len(en_vocab)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "train_loader, train_dataset = get_cn_en_dataloader(seq_len, batch_size, device)\n",
    "print('length of train_loader', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1]) torch.Size([7, 1])\n",
      "[2, 19, 670, 669, 17, 18, 5, 3]\n",
      "['<sos>', '我', '才', '刚', '开', '始', ' 。', '<eos>']\n",
      "[2, 12, 27, 667, 1790, 5, 3]\n",
      "['<sos>', 'I', \"'m\", 'just', 'beginning', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_loader):\n",
    "    print(item[0].shape, item[1].shape)\n",
    "\n",
    "    print([idx.item() for idx in item[0]])\n",
    "    print(to_sentence(item[0], True))\n",
    "    print([idx.item() for idx in item[1]])\n",
    "    print(to_sentence(item[1], False))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(1)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        # repeat decoder hidden state src_len times, hidden = [batch size, seq len, hidden size]\n",
    "        hidden = hidden.repeat(1, seq_len, 1)\n",
    "\n",
    "        # [batch size, seq len, hidden size * 2]\n",
    "        outputs_cat_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "\n",
    "        # energy = [batch size, src len, hidden size]\n",
    "        energy = torch.tanh(self.attn(outputs_cat_hidden))\n",
    "\n",
    "        # attention= [batch size, src len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return self.softmax(attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 3, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # -> [batch size, seq len, hidden size]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # input = [batch size, 1]\n",
    "        # hidden = [1, batch size, hidden_size]\n",
    "        # encoder_outputs = [src len, batch size, hidden_size]\n",
    "\n",
    "        # embedded = [batch size, hidden_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, hidden_size]\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "\n",
    "        # a = [batch size, src len]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, 1, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        # weighted = [batch size, 1, hidden_size]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # weighted = [1, batch size, hidden_size]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "\n",
    "        # rnn_input = [1, batch size, hidden_size * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "\n",
    "        # output = [seq len, batch size, hidden_size]\n",
    "        # hidden = [n_layers, batch size, hidden_size]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, dec hid dim]\n",
    "        # hidden = [1, batch size, dec hid dim]\n",
    "        # this also means that output == hidden\n",
    "        # assert (output == hidden).all()\n",
    "\n",
    "        # embedded = [batch size, hidden_size]\n",
    "        embedded = embedded.squeeze(0)\n",
    "        # output = [batch size, hidden_size]\n",
    "        output = output.squeeze(0)\n",
    "        # weighted = [batch size, hidden_size]\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        # prediction = [batch size, output size]\n",
    "        prediction = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        return prediction, hidden, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    decoder_input = input_tensor[0]\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            target = target_tensor[di]\n",
    "            loss += criterion(decoder_output, target)\n",
    "            decoder_input = target  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(1).detach()  # detach from history as input\n",
    "\n",
    "            target = target_tensor[di]\n",
    "            loss += criterion(decoder_output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, train_loader, print_every=100, plot_every=10, learning_rate=0.0005):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.CrossEntropyLoss(ignore_index=cn_vocab['<pad>'])\n",
    "    n_iters = len(train_loader)\n",
    "\n",
    "    for iter, data in enumerate(train_loader, 1):\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    return plot_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch 1--------------\n",
      "0m 4s (- 0m 42s) (200 9%) 1.8835\n",
      "0m 8s (- 0m 38s) (400 18%) 0.0394\n",
      "0m 12s (- 0m 34s) (600 27%) 0.0187\n",
      "0m 17s (- 0m 29s) (800 36%) 0.0123\n",
      "0m 21s (- 0m 25s) (1000 45%) 0.0097\n",
      "0m 25s (- 0m 21s) (1200 54%) 0.0074\n",
      "0m 30s (- 0m 17s) (1400 63%) 0.0062\n",
      "0m 34s (- 0m 13s) (1600 72%) 0.0051\n",
      "0m 38s (- 0m 8s) (1800 81%) 0.0046\n",
      "0m 43s (- 0m 4s) (2000 90%) 0.0040\n",
      "0m 47s (- 0m 0s) (2200 99%) 0.0037\n",
      "--------------epoch 2--------------\n",
      "0m 4s (- 0m 43s) (200 9%) 0.0032\n",
      "0m 8s (- 0m 39s) (400 18%) 0.0031\n",
      "0m 13s (- 0m 35s) (600 27%) 0.0030\n",
      "0m 17s (- 0m 31s) (800 36%) 0.0027\n",
      "0m 22s (- 0m 26s) (1000 45%) 0.0026\n",
      "0m 26s (- 0m 22s) (1200 54%) 0.0024\n",
      "0m 31s (- 0m 18s) (1400 63%) 0.0022\n",
      "0m 35s (- 0m 13s) (1600 72%) 0.0021\n",
      "0m 40s (- 0m 9s) (1800 81%) 0.0020\n",
      "0m 44s (- 0m 4s) (2000 90%) 0.0018\n",
      "0m 49s (- 0m 0s) (2200 99%) 0.0016\n",
      "--------------epoch 3--------------\n",
      "0m 4s (- 0m 44s) (200 9%) 0.0017\n",
      "0m 9s (- 0m 40s) (400 18%) 0.0015\n",
      "0m 13s (- 0m 36s) (600 27%) 0.0016\n",
      "0m 18s (- 0m 31s) (800 36%) 0.0016\n",
      "0m 22s (- 0m 27s) (1000 45%) 0.0014\n",
      "0m 27s (- 0m 22s) (1200 54%) 0.0014\n",
      "0m 31s (- 0m 18s) (1400 63%) 0.0014\n",
      "0m 36s (- 0m 13s) (1600 72%) 0.0012\n",
      "0m 41s (- 0m 9s) (1800 81%) 0.0012\n",
      "0m 46s (- 0m 4s) (2000 90%) 0.0012\n",
      "0m 51s (- 0m 0s) (2200 99%) 0.0012\n",
      "--------------epoch 4--------------\n",
      "0m 4s (- 0m 48s) (200 9%) 0.0010\n",
      "0m 9s (- 0m 43s) (400 18%) 0.0011\n",
      "0m 14s (- 0m 38s) (600 27%) 0.0009\n",
      "0m 19s (- 0m 33s) (800 36%) 0.0010\n",
      "0m 23s (- 0m 28s) (1000 45%) 0.0010\n",
      "0m 28s (- 0m 23s) (1200 54%) 0.0010\n",
      "0m 33s (- 0m 19s) (1400 63%) 0.0009\n",
      "0m 38s (- 0m 14s) (1600 72%) 0.0010\n",
      "0m 42s (- 0m 9s) (1800 81%) 0.0009\n",
      "0m 47s (- 0m 4s) (2000 90%) 0.0009\n",
      "0m 52s (- 0m 0s) (2200 99%) 0.0008\n",
      "--------------epoch 5--------------\n",
      "0m 4s (- 0m 47s) (200 9%) 0.0008\n",
      "0m 9s (- 0m 42s) (400 18%) 0.0008\n",
      "0m 14s (- 0m 39s) (600 27%) 0.0007\n",
      "0m 19s (- 0m 34s) (800 36%) 0.0008\n",
      "0m 25s (- 0m 30s) (1000 45%) 0.0008\n",
      "0m 30s (- 0m 25s) (1200 54%) 0.0008\n",
      "0m 36s (- 0m 21s) (1400 63%) 0.0008\n",
      "0m 42s (- 0m 16s) (1600 72%) 0.0007\n",
      "0m 49s (- 0m 11s) (1800 81%) 0.0007\n",
      "0m 56s (- 0m 5s) (2000 90%) 0.0007\n",
      "1m 3s (- 0m 0s) (2200 99%) 0.0007\n",
      "--------------epoch 6--------------\n",
      "0m 6s (- 1m 9s) (200 9%) 0.0006\n",
      "0m 13s (- 1m 1s) (400 18%) 0.0006\n",
      "0m 19s (- 0m 52s) (600 27%) 0.0007\n",
      "0m 25s (- 0m 45s) (800 36%) 0.0006\n",
      "0m 31s (- 0m 38s) (1000 45%) 0.0006\n",
      "0m 37s (- 0m 31s) (1200 54%) 0.0006\n",
      "0m 43s (- 0m 25s) (1400 63%) 0.0006\n",
      "0m 49s (- 0m 18s) (1600 72%) 0.0006\n",
      "0m 55s (- 0m 12s) (1800 81%) 0.0006\n",
      "1m 1s (- 0m 6s) (2000 90%) 0.0005\n",
      "1m 7s (- 0m 0s) (2200 99%) 0.0007\n",
      "--------------epoch 7--------------\n",
      "0m 5s (- 0m 58s) (200 9%) 0.0005\n",
      "0m 11s (- 0m 53s) (400 18%) 0.0005\n",
      "0m 17s (- 0m 47s) (600 27%) 0.0006\n",
      "0m 23s (- 0m 41s) (800 36%) 0.0005\n",
      "0m 29s (- 0m 35s) (1000 45%) 0.0005\n",
      "0m 35s (- 0m 29s) (1200 54%) 0.0005\n",
      "0m 40s (- 0m 23s) (1400 63%) 0.0005\n",
      "0m 46s (- 0m 17s) (1600 72%) 0.0005\n",
      "0m 52s (- 0m 11s) (1800 81%) 0.0005\n",
      "0m 57s (- 0m 5s) (2000 90%) 0.0005\n",
      "1m 2s (- 0m 0s) (2200 99%) 0.0005\n",
      "--------------epoch 8--------------\n",
      "0m 5s (- 0m 53s) (200 9%) 0.0005\n",
      "0m 10s (- 0m 49s) (400 18%) 0.0004\n",
      "0m 16s (- 0m 44s) (600 27%) 0.0004\n",
      "0m 22s (- 0m 39s) (800 36%) 0.0004\n",
      "0m 27s (- 0m 33s) (1000 45%) 0.0005\n",
      "0m 33s (- 0m 27s) (1200 54%) 0.0004\n",
      "0m 38s (- 0m 22s) (1400 63%) 0.0005\n",
      "0m 44s (- 0m 16s) (1600 72%) 0.0004\n",
      "0m 49s (- 0m 11s) (1800 81%) 0.0004\n",
      "0m 55s (- 0m 5s) (2000 90%) 0.0005\n",
      "1m 0s (- 0m 0s) (2200 99%) 0.0004\n",
      "--------------epoch 9--------------\n",
      "0m 5s (- 0m 54s) (200 9%) 0.0004\n",
      "0m 10s (- 0m 48s) (400 18%) 0.0004\n",
      "0m 16s (- 0m 42s) (600 27%) 0.0004\n",
      "0m 21s (- 0m 37s) (800 36%) 0.0004\n",
      "0m 26s (- 0m 32s) (1000 45%) 0.0004\n",
      "0m 32s (- 0m 26s) (1200 54%) 0.0004\n",
      "0m 37s (- 0m 21s) (1400 63%) 0.0004\n",
      "0m 42s (- 0m 16s) (1600 72%) 0.0004\n",
      "0m 48s (- 0m 10s) (1800 81%) 0.0003\n",
      "0m 54s (- 0m 5s) (2000 90%) 0.0004\n",
      "0m 59s (- 0m 0s) (2200 99%) 0.0004\n",
      "--------------epoch 10--------------\n",
      "0m 5s (- 0m 52s) (200 9%) 0.0003\n",
      "0m 10s (- 0m 46s) (400 18%) 0.0004\n",
      "0m 14s (- 0m 39s) (600 27%) 0.0004\n",
      "0m 19s (- 0m 34s) (800 36%) 0.0004\n",
      "0m 24s (- 0m 29s) (1000 45%) 0.0004\n",
      "0m 29s (- 0m 24s) (1200 54%) 0.0004\n",
      "0m 34s (- 0m 20s) (1400 63%) 0.0003\n",
      "0m 40s (- 0m 15s) (1600 72%) 0.0003\n",
      "0m 45s (- 0m 10s) (1800 81%) 0.0003\n",
      "0m 51s (- 0m 5s) (2000 90%) 0.0003\n",
      "0m 56s (- 0m 0s) (2200 99%) 0.0004\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = Encoder(cn_vocab_size, hidden_size).to(device)\n",
    "decoder1 = Decoder(hidden_size, en_vocab_size).to(device)\n",
    "\n",
    "train_losses = []\n",
    "for i in range(10):\n",
    "    print('--------------epoch %d--------------' % (i + 1))\n",
    "    losses = trainIters(encoder1, decoder1, train_loader, print_every=200)\n",
    "    train_losses = train_losses + losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "showPlot(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path = './models/seq2seq6-encoder.pt'\n",
    "decoder_path = './models/seq2seq6-decoder.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), encoder_path)\n",
    "torch.save(decoder1.state_dict(), decoder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=seq_len):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = train_dataset.tokenize_sentence(sentence, True).unsqueeze(0)\n",
    "        input_length = input_tensor.size(1)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        decoder_input = torch.tensor([cn_vocab['<sos>']], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, input_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_attentions[di] = decoder_attention.squeeze().data\n",
    "            if topi.item() == en_vocab['<eos>']:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(en_vocab.lookup_token(topi.item()))\n",
    "\n",
    "            decoder_input = topi.squeeze(1).detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(train_dataset.pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2 = Encoder(cn_vocab_size, hidden_size).to(device)\n",
    "decoder2 = Decoder(hidden_size, en_vocab_size).to(device)\n",
    "\n",
    "encoder2.load_state_dict(torch.load(encoder_path))\n",
    "decoder2.load_state_dict(torch.load(decoder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 我累得不能再累了。\n",
      "= I'm as tired as tired can be.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 1 but got size 11 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluateRandomly(encoder2, decoder2)\n",
      "\u001b[1;32m/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb Cell 16\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m, pair[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m, pair[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output_words, _ \u001b[39m=\u001b[39m evaluate(encoder, decoder, pair[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m output_sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(output_words)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, output_sentence)\n",
      "\u001b[1;32m/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb Cell 16\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m decoder_attentions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_length, input_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m di \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_length):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     decoder_output, decoder_hidden, decoder_attention \u001b[39m=\u001b[39m decoder(decoder_input, decoder_hidden, encoder_outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     topv, topi \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtopk(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     decoder_attentions[di] \u001b[39m=\u001b[39m decoder_attention\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb Cell 16\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m weighted \u001b[39m=\u001b[39m weighted\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# rnn_input = [1, batch size, hidden_size * 2]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((embedded, weighted), dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# output = [seq len, batch size, hidden_size]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# hidden = [n_layers, batch size, hidden_size]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/nlps/nlp/rnn/seq2seq-translation-6-1.ipynb#X33sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(rnn_input, hidden)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 11 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder2, decoder2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(encoder2, decoder2, \"我不确定。\")\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)\n",
    "plt.matshow(attentions.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showAttention(input_sentence, output_words, attentions):\n",
    "#     # Set up figure with colorbar\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "#     fig.colorbar(cax)\n",
    "\n",
    "#     # Set up axes\n",
    "#     ax.set_xticklabels([''] + list(input_sentence) +\n",
    "#                        ['<EOS>'], rotation=90)\n",
    "#     ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "#     # Show label at every tick\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def evaluateAndShowAttention(input_sentence):\n",
    "#     output_words, attentions = evaluate(encoder2, decoder2, input_sentence)\n",
    "#     print('input =', input_sentence)\n",
    "#     print('output =', ' '.join(output_words))\n",
    "#     showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "# evaluateAndShowAttention(\"我不确定。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_vocab.get_stoi()['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4670fb8ce984b94d35f99e9f6e3660635f3fa9b149816d0027b93d0ab56905c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
